import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
from html import unescape

HEADERS = {"User-Agent": "Mozilla/5.0"}
KEYWORDS = ["investor", "financial", "earnings", "results", "reports", "10-q", "10-k", "sec", "statement", "quarter"]
SKIP_EXTENSIONS = [".pdf", ".xls", ".xlsx", ".doc", ".docx", ".zip"]

def fetch_html(url):
    try:
        res = requests.get(url, headers=HEADERS, timeout=10)
        return res.text
    except Exception as e:
        st.error(f"Failed to fetch {url}: {e}")
        return ""

def extract_financial_links(base_url, html):
    soup = BeautifulSoup(html, "lxml")
    links = set()
    for tag in soup.find_all("a", href=True):
        href = tag["href"].strip()
        if any(keyword in href.lower() for keyword in KEYWORDS):
            full_url = href if href.startswith("http") else requests.compat.urljoin(base_url, href)
            if not any(full_url.lower().endswith(ext) for ext in SKIP_EXTENSIONS):
                links.add(full_url)
    return list(links)

def extract_tables_and_text(html):
    soup = BeautifulSoup(html, "lxml")
    try:
        tables = pd.read_html(str(soup))
    except:
        tables = []
    return tables, soup.get_text()

def clean_text(raw_text: str) -> str:
    text = unescape(raw_text)

    # Remove markdown asterisks and normalize dashes
    text = re.sub(r"\*{1,3}", "", text)
    text = re.sub(r"[–—]", " - ", text)

    # Add spacing to stuck-together tokens
    text = re.sub(r"(?<=[a-z])(?=[A-Z])", " ", text)
    text = re.sub(r"([a-zA-Z])(\d)", r"\1 \2", text)
    text = re.sub(r"(\d)([a-zA-Z])", r"\1 \2", text)
    text = re.sub(r"([a-z]{3,})([A-Z][a-z]+)", r"\1 \2", text)

    # Normalize whitespace
    text = re.sub(r"\s+", " ", text).strip()

    # Inject structure using financial section cues
    section_headers = [
        ("Financial Results", r"(financial results|net earnings|adjusted earnings|gross margin|cash on hand)", re.IGNORECASE),
        ("Earnings Highlights", r"(earnings highlights|revenue|profit|loss|income)", re.IGNORECASE),
        ("Key Updates", r"(key updates|developments|milestones|progress)", re.IGNORECASE),
        ("Outlook", r"(outlook|guidance|forecast|expectations)", re.IGNORECASE),
    ]
    for title, pattern, flags in section_headers:
        text = re.sub(pattern, f"\n\n**{title}:**", text, flags=flags)

    disclaimer = "\n\n---\nDisclaimer: This summary was generated by an AI model and may contain inaccuracies. Always refer to the official company filings or investor reports for reliable financial information."
    return text + disclaimer

def ai_extract_summary(text):
    prompt = f"""You are a financial analyst assistant. Summarize the company's financial results, earnings, and key updates based on the following text:

{text}"""
    try:
        api_key = st.secrets["together"]["api_key"]
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        payload = {
            "model": "meta-llama/Llama-3-70b-chat-hf",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            "max_tokens": 1000
        }
        res = requests.post("https://api.together.xyz/v1/chat/completions", headers=headers, json=payload)
        if res.status_code == 200:
            raw = res.json()["choices"][0]["message"]["content"].strip()
            return clean_text(raw)
        else:
            return f"API failure: {res.status_code} - {res.text}"
    except Exception as e:
        return f"Error: {e}"

def run():
    st.title("Financial Data Extractor")
    st.markdown("Paste a public company homepage or investor relations page to automatically extract financial summaries and data tables.")

    url = st.text_input("Enter Company URL", placeholder="https://www.microsoft.com")

    if url:
        with st.spinner("Scanning website..."):
            base_html = fetch_html(url)
            if not base_html:
                return

            subpages = extract_financial_links(url, base_html)
            subpages = list(dict.fromkeys(subpages))[:5]

            if not subpages:
                st.warning("No relevant financial subpages found.")
                return

            st.success(f"Found {len(subpages)} financial subpages.")
            results = []

            for sub_url in subpages:
                sub_html = fetch_html(sub_url)
                if not sub_html:
                    continue
                try:
                    tables, text = extract_tables_and_text(sub_html)
                    with st.spinner(f"Analyzing: {sub_url}"):
                        summary = ai_extract_summary(text)
                    results.append((sub_url, summary, tables))
                except Exception as e:
                    st.error(f"Error reading {sub_url}: {e}")

        if results:
            for i, (link, summary, tables) in enumerate(results):
                with st.expander(f"Page {i+1}: {link}", expanded=False):
                    st.markdown(f"[View original page]({link})", unsafe_allow_html=True)
                    st.markdown("#### Summary")
                    st.markdown(summary)

                    if tables:
                        st.markdown("#### Key Tables")
                        for idx, table in enumerate(tables[:2]):
                            st.markdown(f"**Table {idx+1}**")
                            st.dataframe(table)
                    else:
                        st.info("No tables found on this page.")
